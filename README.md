How can we use TLPPO to guide RL. See this paper for an idea: https://arxiv.org/abs/1504.00702 Basically, we want to employ TLPPO often early during training when the agent has no idea what it's doing to solve the task. We can seed TLPPO with a replay buffer of high quality data for this part. As data is collected, RL pays attention to TLPPO and uses behavioral cloning on its state action pairs, learning to copy the trajectories it selects. Eventually, RL will get very good at the task and we will no longer need to engage the planner. In this way, we can move from planning to pure RL. At the start of each episode, we can select if we want to use the RL agent or the TLPPO agent to suggest a plan for the agent to follow. This is like planning-greedy exploration. Instead of epsilon-greedy exploration.
